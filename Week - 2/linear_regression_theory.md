
---

# **ğŸ“˜ Linear Regression â€” Full Theoretical + Mathematical Notes**


# **1ï¸âƒ£ What is Linear Regression?**

Linear Regression is a **supervised learning algorithm** used to model the relationship between:

* **Independent variables (features)** â†’ ( X )
* **Dependent variable (target)** â†’ ( y )

Goal:
ğŸ‘‰ Find a **best-fit straight line** that predicts ( y ) from ( X ).

---

# **2ï¸âƒ£ Types of Linear Regression**

### **1. Simple Linear Regression**

* One feature
* Model:
          y=Î²0â€‹+Î²1â€‹x+Îµ

### **2. Multiple Linear Regression**

* Multiple features
* Model:
          y=Î²0â€‹+Î²1â€‹x1â€‹+Î²2â€‹x2â€‹+â‹¯+Î²nâ€‹xnâ€‹+Îµ
### **3. Polynomial Regression**

* Non-linear relation handled with polynomial features
* Still linear in coefficients.

---

# **3ï¸âƒ£ Assumptions of Linear Regression (Very Important)**

To get reliable results, Linear Regression assumes:

1. **Linearity**
   Relationship between features and output is linear.

2. **Independence**
   Observations are independent.

3. **Homoscedasticity**
   Equal variance of errors.

4. **Normality of Errors**
   Residuals ~ Normal distribution.

5. **No Multicollinearity**
   Features should not be highly correlated.

---

# **4ï¸âƒ£ Mathematical Formulation**

### **Model Equation (Vector Form)**

For multiple regression:

          y=XÎ²+Îµ

Where:

* ( X ) â†’ matrix of features
* ( \beta ) â†’ coefficients
* ( y ) â†’ target
* ( \varepsilon ) â†’ error term

---

# **5ï¸âƒ£ Cost Function â€“ Mean Squared Error (MSE)**

Linear Regression minimizes the **sum of squared errors**.

          J(Î²)=2m1â€‹i=1âˆ‘mâ€‹(yiâ€‹âˆ’y^â€‹iâ€‹)2

Where:

* ğ‘š = number of samples
* y_i  = actual value
* ( \hat{y}_i = X\beta ) = predicted value

Goal:
ğŸ‘‰ **Minimize** ( J(\beta) )

---

# **6ï¸âƒ£ Finding Best Coefficients (Î²)**

### **Method 1: Normal Equation**

Closed-form solution (no gradient descent needed):

          Î²=(XTX)âˆ’1XTy

Works well when:

* small dataset
* features < 10,000

Fails when:

* matrix becomes non-invertible
* large dataset â†’ slow

---

### **Method 2: Gradient Descent**

Iterative optimization:

          Î²:=Î²âˆ’Î±âˆ‚Î²âˆ‚J(Î²)â€‹

Where:

* ( \alpha ) = learning rate
* Compute gradient:

          [
          \frac{\partial J}{\partial\beta}=-\frac{1}{m}X^T(y-X\beta)
          ]

Update rule:

[
\beta := \beta + \alpha \frac{1}{m}X^T(y-X\beta)
]

Repeat until convergence.

---

# **7ï¸âƒ£ Evaluation Metrics**

### **1. RÂ² Score**

Measures how much variance in y is explained.

[
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
]

Where:

* ( SS_{res} = \sum (y - \hat{y})^2 )
* ( SS_{tot} = \sum (y - \bar{y})^2 )

---

### **2. Adjusted RÂ²**

Penalizes extra features.

[
R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-k-1}
]

Where:

* ( n ) â†’ samples
* ( k ) â†’ features

---

### **3. RMSE: Root Mean Squared Error**

[
RMSE = \sqrt{\frac{1}{m}\sum (y-\hat{y})^2}
]

---

# **8ï¸âƒ£ Gradient Descent Variants**

1. **Batch GD** â€“ uses whole data
2. **Stochastic GD** â€“ uses one example
3. **Mini-Batch GD** â€“ uses small batches (most used)

---

# **9ï¸âƒ£ Problems with Linear Regression**

1. **Outliers influence model heavily**
2. **Multicollinearity â†’ unstable coefficients**
3. **Underfitting if relationship is non-linear**

---

# **ğŸ”Ÿ Regularization in Linear Regression**

Used to reduce overfitting by penalizing large coefficients.

### **1. Ridge Regression (L2)**

          J(Î²)=MSE+Î»âˆ‘Î²i2â€‹

### **2. Lasso Regression (L1)**

          J(Î²)=MSE+Î»âˆ‘âˆ£Î²iâ€‹âˆ£

### **3. Elastic Net**

Combination of L1 + L2

---

# **1ï¸âƒ£1ï¸âƒ£ Geometric Interpretation**

Linear Regression finds a **hyperplane** in n-dimensional space.

Example:

* 1 feature â†’ line
* 2 features â†’ plane
* n features â†’ n-dimensional hyperplane

Goal: minimize perpendicular distance between points and that hyperplane.

---

# **1ï¸âƒ£2ï¸âƒ£ Statistical Interpretation**

[
\beta_1 = \frac{Cov(X, Y)}{Var(X)}
]

Intercept:
[
\beta_0 = \bar{y} - \beta_1\bar{x}
]

This shows:

* slope depends on covariance
* intercept shifts line to match mean

---

# **1ï¸âƒ£3ï¸âƒ£ Error / Residual Analysis**

Residual =

          eiâ€‹=yiâ€‹âˆ’y^â€‹iâ€‹

Good model:

* residuals randomly distributed
* no pattern
* constant variance

---

# **1ï¸âƒ£4ï¸âƒ£ When to Use Linear Regression**

Use when:
âœ“ Relationship approx linear
âœ“ Data clean, no extreme outliers
âœ“ Interpretability needed

Don't use when:
âœ— Complex non-linear relations
âœ— High multicollinearity
âœ— Many categorical variables without encoding

---

# **1ï¸âƒ£5ï¸âƒ£ Summary for Notes**

* Linear Regression predicts output using straight line.
* Uses MSE cost function.
* Coefficients: Normal Equation / Gradient Descent
* Evaluation: RÂ², RMSE
* Assumptions must be satisfied
* Regularization prevents overfitting
* Easy to interpret, fast, widely used

---


