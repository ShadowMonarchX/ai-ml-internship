# ðŸ“˜ Week 4 â€“ Deep Learning & Transformer Architectures  
**AI/ML Internship â€“ Final Work Summary**  
ðŸ‘¨â€ðŸ’» **Intern:** Jenish Shekhada  

---

## ðŸ“Œ Overview

Week 4 focused on **Deep Learning architectures**, progressing from **Artificial Neural Networks (ANNs)** to **Convolutional Neural Networks (CNNs)**, **Recurrent Neural Networks**, and finally **Transformer architectures** used in modern NLP systems.

This week emphasized:
- Hands-on **PyTorch implementations**
- Strong **theoretical understanding**
- **Research paper reading**
- **Architecture visualization and comparison**

---

## ðŸ§  Key Learning Areas

- Artificial Neural Networks (ANN)
- Convolutional Neural Networks (CNN)
- Image Classification using CIFAR-10
- Recurrent Neural Networks (RNN, LSTM, GRU)
- Vanishing Gradient Problem & Solutions
- Attention Mechanism
- Transformer Architecture (Encoderâ€“Decoder)
- BERT vs GPT
- Research Paper: *Attention Is All You Need*

---

## ðŸ“‚ Work Breakdown

---

### ðŸ”¹ Breast Cancer Detection using ANN (PyTorch)
- **Type:** Tutorial / Code  
- Built an **Artificial Neural Network** for binary classification.
- Implemented forward propagation, loss calculation, and backpropagation.
- Understood ANN usage in medical diagnosis tasks.

---

### ðŸ”¹ Convolutional Neural Networks (CNN) with PyTorch
- **Type:** Tutorial / Theory  
- Studied CNN fundamentals:
  - Convolution layers
  - Pooling layers
  - Filters, stride, and padding
- Learned why CNNs are effective for image data.

---

### ðŸ”¹ CIFAR-10 Image Classification using CNN
- **Type:** Tutorial / Code  
- Implemented a CNN model using PyTorch.
- Trained and evaluated the model on the CIFAR-10 dataset.
- Practiced data loading, training loops, and accuracy evaluation.

---

### ðŸ”¹ Recurrent Neural Networks & LSTM Basics
- **Type:** Tutorial / Theory  
- Learned how RNNs process sequential data.
- Identified limitations of RNNs with long-term dependencies.
- Studied LSTM architecture and gating mechanisms.

---

### ðŸ”¹ Sequence Modeling Techniques
- **Type:** Tutorial / Theory  

#### âœ” RNN
- Time-step unrolling
- Hidden state propagation

#### âœ” LSTM
- Forget gate, input gate, output gate
- Solving long-term dependency issues

#### âœ” GRU
- Update and reset gates
- Comparison with LSTM in terms of efficiency

---

### ðŸ”¹ Vanishing Gradient Problem
- **Type:** Tutorial / Theory  
- Understood why gradients vanish in deep neural networks.
- Analyzed impact on deep CNNs and RNN-based models.

---

### ðŸ”¹ Solutions to Vanishing Gradient Problem
- **Type:** Tutorial / Theory  
- Techniques studied:
  - ReLU activation
  - Batch Normalization
  - Residual Connections
  - LSTM & GRU architectures

---

### ðŸ”¹ Practical Implementation of Gradient Stability
- **Type:** Tutorial / Code  
- Implemented techniques to improve gradient flow.
- Observed improved training stability in deep models.

---

### ðŸ”¹ Transformer Architecture â€“ Input & Preprocessing
- **Type:** Tutorial / Theory  
- Tokenization
- Word embeddings
- Positional encoding
- Transformer input pipeline

---

### ðŸ”¹ Attention Mechanism (Context Formation)
- **Type:** Tutorial / Theory  
- Scaled Dot-Product Attention
- Query, Key, and Value representation
- Multi-Head Attention
- Context vector formation

---

### ðŸ”¹ Research Paper Study â€“ *Attention Is All You Need*
- **Type:** Tutorial / Theory  
- Studied:
  - Encoderâ€“Decoder architecture
  - Self-attention vs recurrence
  - Parallelization benefits of transformers

---

### ðŸ”¹ Encoder vs Decoder in Transformers
- **Type:** Tutorial / Theory  

| Architecture Type | Example Model | Purpose |
|------------------|--------------|---------|
| Encoder-only | BERT | Language Understanding |
| Decoder-only | GPT | Text Generation |
| Encoderâ€“Decoder | T5 | Seq-to-Seq Tasks |

---

### ðŸ”¹ BERT and GPT â€“ Comparative Study
- **Type:** Tutorial / Theory  
- BERT:
  - Masked Language Modeling
  - Bidirectional context
- GPT:
  - Autoregressive modeling
  - Next-token prediction
- Compared training objectives, architectures, and use cases.

---

## ðŸš€ Skills Gained

- PyTorch deep learning workflows
- CNN & RNN architecture design
- Sequence modeling fundamentals
- Transformer deep architecture understanding
- Attention mechanism mastery
- Research paper analysis
- NLP model comparison (BERT vs GPT)

---

## âœ… Conclusion

Week 4 provided a strong foundation in **deep learning and transformer-based architectures**, connecting traditional neural networks to modern NLP systems. This work strengthened both **practical implementation skills** and **theoretical understanding**, preparing me for advanced AI and NLP tasks.

---

ðŸ“Œ **Next Focus:**  
> Advanced transformer models, fine-tuning large language models, and applied NLP use cases.
