{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a124ce21-4087-4b27-8406-aeb06a6e156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.24.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.9 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.9.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (2.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.9.1-cp312-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.24.1-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.9.1-cp312-cp312-macosx_11_0_arm64.whl (808 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m808.1/808.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.6-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, networkx, torch, torchvision, torchaudio\n",
      "Successfully installed mpmath-1.3.0 networkx-3.6 sympy-1.14.0 torch-2.9.1 torchaudio-2.9.1 torchvision-0.24.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "070ba1cb-84d8-4c77-bfe8-964329e83453",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db955c19-b7ff-4763-9f2c-d94616702c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = 'merged_dataset.csv'   # update if needed\n",
    "FEATURES = ['StudyHours', 'Attendance', 'Motivation', 'ExamScore']  # choose 2-5 features\n",
    "TARGET = 'FinalGrade'\n",
    "HIDDEN_NEURONS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 200           # you can increase later\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adc6c004-0a70-407a-a933-9514138df819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11236cb50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26d4b118-a624-4c20-b022-42781ac4310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (14003, 16)\n",
      "Available columns: ['StudyHours', 'Attendance', 'Resources', 'Extracurricular', 'Motivation', 'Internet', 'Gender', 'Age', 'LearningStyle', 'OnlineCourses', 'Discussions', 'AssignmentCompletion', 'ExamScore', 'EduTech', 'StressLevel', 'FinalGrade']\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {CSV_PATH}\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# Make sure features and target exist\n",
    "for c in FEATURES + [TARGET]:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Column '{c}' not found in dataset. Choose different FEATURES/TARGET.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "536a945d-3cc4-4401-b6df-48e4c27f360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[FEATURES + [TARGET]].copy()\n",
    "# Fill missing values with median (simple strategy)\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "# Inputs and target arrays\n",
    "X = data[FEATURES].astype(float).values\n",
    "y = data[[TARGET]].astype(float).values  # shape (n,1)\n",
    "\n",
    "# Scale inputs and target (helps training)\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_scaled, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_t = torch.from_numpy(X_train.astype(np.float32))\n",
    "y_train_t = torch.from_numpy(y_train.astype(np.float32))\n",
    "X_test_t = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_test_t = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51e302eb-c55e-420c-b896-5a539d2fcb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Manual single-sample forward + backward demo ---\n",
      "Manual forward -> predicted (raw scale): -0.1904, actual: 3.0000\n",
      "Manual loss (0.5 * (y - yhat)^2): 5.089429\n",
      "Shapes: W1 (3, 4) dW1 (3, 4) W2 (1, 3) dW2 (1, 3)\n",
      "--- end manual demo ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def manual_single_sample_demo():\n",
    "    print(\"\\n--- Manual single-sample forward + backward demo ---\")\n",
    "    # pick first raw (unscaled) sample for clarity\n",
    "    X0 = X[0].reshape(-1, 1)  # shape (features, 1)\n",
    "    y0 = y[0, 0]\n",
    "\n",
    "    # small random initial weights (hidden layer 3 neurons for the demo)\n",
    "    hidden_demo = 3\n",
    "    W1 = np.random.randn(hidden_demo, X0.shape[0]) * 0.1   # (hidden, input)\n",
    "    b1 = np.zeros((hidden_demo, 1))\n",
    "    W2 = np.random.randn(1, hidden_demo) * 0.1             # (output, hidden)\n",
    "    b2 = np.zeros((1, 1))\n",
    "\n",
    "    # Forward\n",
    "    Z1 = W1.dot(X0) + b1                 # (hidden,1)\n",
    "    A1 = np.maximum(0, Z1)               # ReLU\n",
    "    Z2 = W2.dot(A1) + b2                 # (1,1)\n",
    "    y_hat = Z2[0, 0]\n",
    "    print(f\"Manual forward -> predicted (raw scale): {y_hat:.4f}, actual: {y0:.4f}\")\n",
    "\n",
    "    # Loss (MSE for single sample): 0.5*(y-yhat)^2  (some use 1/2 for simpler derivative)\n",
    "    loss = 0.5 * (y0 - y_hat) ** 2\n",
    "    print(f\"Manual loss (0.5 * (y - yhat)^2): {loss:.6f}\")\n",
    "\n",
    "    # Backward (manual gradients)\n",
    "    # dL/dyhat = -(y - yhat)\n",
    "    dL_dyhat = -(y0 - y_hat)\n",
    "    # dL/dW2 = dL/dyhat * d(yhat)/dW2 = dL_dyhat * A1^T\n",
    "    dW2 = dL_dyhat * A1.T                # shape (1, hidden)\n",
    "    db2 = dL_dyhat\n",
    "    # Backprop into hidden: dL/dA1 = dL/dyhat * W2^T\n",
    "    dA1 = dL_dyhat * W2.T                # (hidden,1)\n",
    "    # dA1/dZ1 = ReLU'(Z1) -> 1 if Z1>0 else 0\n",
    "    dZ1 = dA1 * (Z1 > 0).astype(float)\n",
    "    # dL/dW1 = dZ1 * X0^T\n",
    "    dW1 = dZ1.dot(X0.T)\n",
    "\n",
    "    print(\"Shapes: W1\", W1.shape, \"dW1\", dW1.shape, \"W2\", W2.shape, \"dW2\", dW2.shape)\n",
    "    print(\"--- end manual demo ---\\n\")\n",
    "\n",
    "# Run manual demo (optional)\n",
    "manual_single_sample_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bce446d1-4a87-4e7a-8e2f-f38472f4d2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SimpleANN(\n",
      "  (fc1): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (act): ReLU()\n",
      "  (fc2): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_neurons, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_neurons)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_neurons, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleANN(input_dim=X_train.shape[1], hidden_neurons=HIDDEN_NEURONS, output_dim=1)\n",
    "print(\"Model:\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6b205c8-f69d-4607-9e20-00e07da0e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ebf2ae-b950-4864-8b0e-baeb6d9ef095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Train MSE (scaled target): 0.093148\n",
      "Epoch 20/200 - Train MSE (scaled target): 0.044044\n",
      "Epoch 40/200 - Train MSE (scaled target): 0.018952\n",
      "Epoch 60/200 - Train MSE (scaled target): 0.011780\n",
      "Epoch 80/200 - Train MSE (scaled target): 0.010735\n",
      "Epoch 100/200 - Train MSE (scaled target): 0.009356\n",
      "Epoch 120/200 - Train MSE (scaled target): 0.009251\n",
      "Epoch 140/200 - Train MSE (scaled target): 0.005814\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_losses.append(loss.item())\n",
    "    epoch_loss = float(np.mean(batch_losses))\n",
    "    loss_history.append(epoch_loss)\n",
    "    if epoch % max(1, EPOCHS//10) == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} - Train MSE (scaled target): {epoch_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539a507-ec17-4b2f-bf7a-10258ef5351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_scaled = model(X_test_t).cpu().numpy()\n",
    "\n",
    "preds = scaler_y.inverse_transform(preds_scaled)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e8808-5c91-4898-9ab9-9698eee533b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample predictions (first 10 test samples):\")\n",
    "n_show = min(10, len(preds))\n",
    "for i in range(n_show):\n",
    "    print(f\"Predicted: {preds[i,0]:.3f} \\t Actual: {y_test_orig[i,0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02eb792-2de9-4be8-8089-85ab86281e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2  = r2_score(y_test_orig, preds)\n",
    "mae = mean_absolute_error(y_test_orig, preds)\n",
    "mse = mean_squared_error(y_test_orig, preds)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"\\nðŸ“Œ MODEL ACCURACY METRICS\")\n",
    "print(f\"RÂ² Score:  {r2:.4f}\")\n",
    "print(f\"MAE:       {mae:.4f}\")\n",
    "print(f\"MSE:       {mse:.4f}\")\n",
    "print(f\"RMSE:      {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc358808-f41a-4e42-be11-6ae3149de7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(y_test_orig, preds)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs Actual Values\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae7c30-2a2c-4be5-93f3-19beb431b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(y_test_orig[:50], label=\"Actual\")\n",
    "plt.plot(preds[:50], label=\"Predicted\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Actual vs Predicted (First 50 Samples)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0fed6-80d9-489a-88a6-83a15f2dd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, EPOCHS+1), loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE (scaled target)\")\n",
    "plt.title(\"Training Loss vs Epochs\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731eba5c-440d-4653-8445-b70f82c545f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('outputs_ANN', exist_ok=True)\n",
    "torch.save(model.state_dict(), 'outputs_ANN/ann_model.pth')\n",
    "\n",
    "report_txt = 'outputs_ANN/ann_report.txt'\n",
    "with open(report_txt, 'w') as f:\n",
    "    f.write(\"ANN Student Scores Training Report\\n\")\n",
    "    f.write(f\"Features: {FEATURES}\\nTarget: {TARGET}\\n\")\n",
    "    f.write(f\"Model: Input {X_train.shape[1]} -> Hidden {HIDDEN_NEURONS} (ReLU) -> Output 1\\n\")\n",
    "    f.write(f\"Epochs: {EPOCHS}, Batch size: {BATCH_SIZE}, LR: {LEARNING_RATE}\\n\")\n",
    "    f.write(f\"Final Train MSE (scaled): {loss_history[-1]:.6f}\\n\\n\")\n",
    "    f.write(\"Sample predictions vs actuals (first 10 test samples):\\n\")\n",
    "    for i in range(n_show):\n",
    "        f.write(f\"Predicted: {preds[i,0]:.3f} \\t Actual: {y_test_orig[i,0]:.3f}\\n\")\n",
    "\n",
    "print(f\"\\nModel saved to: outputs_ANN/ann_model.pth\")\n",
    "print(f\"Report saved to: {report_txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d7f6d-8562-4dce-851c-7506595b2c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
